\section{Introduction} 

This document contains theoretical aspects and actual implementation of various neural nets. 

Structure of this document is organized as follows: in section 1, general introduction of machine learning and neural net will be given. From section 2, each neural net will be studied in detail. Implementation by python and its test results will be given for each neural net. For now\footnote{2017.12.07}, architectures below will be covered in this document\footnote{List from \href{https://en.wikipedia.org/wiki/Types_of_artificial_neural_networks}{wikipedia} and \href{http://www.asimovinstitute.org/neural-network-zoo/}{Asimov Institute}}. 
The list below is sorted in timal order based on the publication date of original pdf, so that reader can follow the historic timeline of artificial neural net development. 

\begin{itemize} 
\item Perceptron/Feedforward Network \cite{perceptron} 
\item Kohonen Network(KN) \cite{KN}
\item Boltzmann Machine(BM) \cite{BM}
\item Restricted BM(RBM) \cite{RBM}
\item Radial Basis Network(RBF) \cite{RBF}
\item AutoEncoder(AE) \cite{AE}
\item Hopfield Network(HN) \cite{HN}
\item Recurrent Neural Network(RNN) \cite{RNN}
\item Support Vector Machine(SVM) \cite{SVM}
\item Long/Short Term Memory(LSTM) \cite{LSTM}
\item Bidirectional RNN(Bi-RNN) \cite{Bi-RNN}
\item Deep Convolutioanl Network(DCNN) \cite{DCNN}
\item Liquid State Machine(LSM) \cite{LSM}
\item Echo State Network(ESN) \cite{ESN}
\item Sparse AE(SAE) \cite{SAE}
\item Deep Belief Network(DBN) \cite{DBN}
\item Denoising AE(DAE) \cite{DAE}
\item Deconvolutional Network(DN) \cite{DN}
\item Variational AE(VAE) \cite{VAE}
\item Markov Chain(MC) \cite{MC}
\item Gated Recurrent Unit(GRU) \cite{GRU}
\item Generative Adversarial Network(GAN) \cite{GAN}
\item Neural Turing Machine(NTM) \cite{NTM}
\item Deep Convolutional Inverse Graphics Network(DCIGN) \cite{DCIGN}
\item Extreme Learning Machine(ELM) \cite{ELM}
\item Deep Residual Network(DRN) \cite{DRN}
\end{itemize}

Implementation for each structures will be uploaded on github\footnote{\href{https://github.com/principia12/NN101}{https://github.com/principia12/NN101}}.

After all implementations are covered, optimization of neural networks will be discussed in two perspectives: application of evolutionary algorithms on topology of neural net architecture and evolutionary algorithm on generatlization of activation function. 
